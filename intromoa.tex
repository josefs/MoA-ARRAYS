% See the ``Article customise'' template for come common customisations
\newcommand{\cat}{+\!\!\!+}
%\newcommand{\cat}{\mbox{$+\!\!+_s$}}
\newcommand{\Take}{\mbox{Take}}
\newcommand{\take}{\,\bigtriangleup\,}
\newcommand{\Drop}{\mbox{Drop}}
\newcommand{\drop}{\,\bigtriangledown\,}
\newcommand{\rshp}{\,\widehat{\Rho}\;}
\newcommand{\Rho}{\,\rho\,}
\newcommand{\Tau}{\,\tau\,}
\newcommand{\Dim}{\,\delta\,}
\newcommand{\transpose}{\bigcirc\!\!\!\!\!\backslash\;}
\newcommand{\kron}{\bigcirc\,\!\!\!\!\!\!\times\;}
\newcommand{\Ravel}{\,\mbox{\tt rav}\,}
\newcommand{\Gradeup}{\,\mbox{\tt gu}\,}
%\newcommand{\cat}{+\!\!\!\!+}

\newtheorem{definition}{Definition}
%\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}

\noindent

MoA, an acoronym for {\em A Mathematics of Arrays\cite{thesis}}, is a theory of arrays with both an 
algebra and a calculus of 
indexing based on shapes, i.e. sizes of each dimension. The index calculus, refered to as {\em The Psi Calculus}, 
gets its names from the {\em Psi}  function which is the foundational function in the theory. These results began with the awareness that Iverson's APL\cite{iverson62}
was inspired by Sylvester's Universal Algebra\cite{sylvester} and the fact that he and Cayley invented matrices.

MoA and the Psi Calculus, as a theory,  have  also been used to describe and verify 
hardware\cite{hardy1,hardy2,fftharry}, describe the partitioning and
scheduling of array operations (and their costs)\cite{mdst93,lee95,aachen96,coffin94,ll2,mcmahon95,haleh98}, 
and the formulation of parallel and distributed processing\cite{Mul03}. 
MoA and $\psi$-Calculus, can be used to abstract complex processor memory layouts 
for tensor/array expressions, reduce these expressions first to a semantic 
normal form (Denotational Normal Form-DNF) then to an Operational Normal Form 
(ONF). 
%
%
%
%\bibliographystyle{plain}
%\bibliography{paper1,hpf,workshop}
\subsection*{Elements of the theory}
\subsection*{\label{indshp} Indexing and Shapes}

The central operation of MoA is the indexing function
\[
                    p \psi A \]
in which a vector of $n$ integers $p$ is used to select an item of 
the $n$-dimensional array $A$.
The operation is generalized to select a partition of $A$, 
so that if $q$ has only $k < n $ components
then
\[                    q \psi A \]
is an array of dimensionality $n-k$ and $q$ selects among the possible 
choices for the first $k$ axes. In MoA zero origin indexing is assumed.
For example, if $A$ is the $3$ by $5$ by $4$ array
\small
\[ \left [
\begin{array}{lrrrr}
0 & 1 &  2 & 3  \\
4 & 5 & 6 & 7 \\
8 & 9 & 10 & 11  \\
12 & 13 & 14 & 15 \\
16 & 17 & 18 & 19 
\end{array} \right ] 
\left [ \begin{array}{rrrr}
20 & 21 & 22 & 23 \\
24 & 25 & 26 & 27 \\
28 & 29 & 30 & 31 \\
32 & 33 & 34 & 35 \\
36 & 37 & 38 & 39 
\end{array} \right ]
\left [ \begin{array}{rrrr}
40 & 41 & 42 & 43 \\
44 & 45 & 46 & 47 \\
48 & 49 & 50 & 51 \\
52 & 53 & 54 & 55 \\
56 & 57 & 58 & 59
\end{array} \right ] \]
\normalsize
then
\[
     <1> \psi A = \left [  \begin{array}{rrrr}
                   20 & 21 & 22 & 23 \\
                   24 & 25 & 26 & 27 \\
                   28 & 29 & 30 & 31 \\
                   32 & 33 & 34 & 35 \\
                   36 & 37 & 38 & 39 
                    \end{array} \right ] \]

\[     <2 \; 1> \psi A \; \; = \; \; < \; 44 \;\; 45 \;\; 46 \;\; 47 \; > \]

\[      <2\; 1\; 3> \psi A \; \; = \; \; 47 \]



	We now introduce notation to permit us to define $\psi$ formally. We will use $A, B, ...$ to denote an array of numbers (integer, real, or complex).
An array's dimensionality will be denoted by $d_A$ and will be assumed to 
be $n$ if not specified.

	The shape of an array $A$, denoted by $s_A$, is a vector of integers of length $d_A$,
each item giving the length of the corresponding axis. The total number of items in an
array, denoted by $t_A$, is equal to the product of the items of the shape. The subscripts
will be omitted in contexts where the meaning is obvious.

A full index is a vector of $n$ integers that describes one position 
in an $n$-dimensional
array. Each item of a full index for $A$ is less than the corresponding item of
$s_A$. There are precisely $t_A$ indices for an array(due to a
zero index origin). A partial index of $A$ is a vector of $0 \leq  k < n $
integers with each item less than the corresponding item of $s_A$.

	We will use a tuple notation (omitting commas) to describe vectors of a fixed length.
For example,
\[                  <i \; j \; k> \]
denotes a vector of length three. $<>$ will denote the empty vector.

	For every $n$-dimensional array $A$, there is a vector of the items of $A$,
which we denote by the corresponding lower case letter, here $a$. The length of the vector of
items is $t_A$. A vector is itself a one-dimensional array, whose shape is the one-item
vector holding the length. Thus, for $a$, the vector of items of $A$, the
shape of $a$ is
\[                     s_a =  \; < t_A> \]
and the number of items or total number of components
$a$\footnote{We also use $\tau a , \delta a,$ and $\rho a$ to denote
total number of components, dimensionality and shape of a. } is
\[                    t_a =  t_A . \]

	The precise mapping of $A$ to $a$ is determined by a one-to-one 
ordering function, $gamma$
\footnote{There are a family of gamma functions: 
$\gamma_{row}, \gamma_{column}, \gamma_{regular sparse}$, etc. In this case we assume row major ordering.}.  
Scalars are introduced as arrays with an empty shape vector.

There are two equivalent ways of describing an array $A$:
\begin{description}
\item[(1)] by its shape and the vector of items, i.e.  $A = \{s_A , a\}$, or
\item[(2)] by its shape and a function that defines the value at every index $p$.
\end{description}
These two forms have been shown to be formally equivalent~\cite{jenk94}.
We wish to use the second form in defining functions on multidimensional
arrays using their Cartesian coordinates (indices). 
The first form is used in describing
address manipulations to achieve effective computation.

To complete our notational conventions, we assume that $p,q,...$ will 
be used to denote
indices or partial indices and that $u,v,..$ will be used to denote 
arbitrary vectors of integers.
In order to describe the $i_{th}$ item of a vector $a$, either $a_i$ or 
$a[i]$ will be used. If $u$ is a
vector of $k$ integers all less than $t_A$, then $a[u]$
will denote the vector of length $k$, whose items are the items of $a$ at 
positions $u_j$, $j = 0,...,k-1.$

Before presenting the formal definition of the $\psi$ indexing function we 
define a few functions on vectors:
\begin{tabbing}

\hspace{.25cm}\= $u \;\cat\; v$ \hspace{1cm} \=		\=catentation of vectors $u$ and $v$ \\
\>$u$ + $v$\>		\>itemwise vector addition assuming $t_u = t_v$ \\
\>$u \times v$ \>	\>	itemwise vector multiplication \\
\>$n$ + $u$, $u$ + $n$ \>	addition of a scalar to each item of a vector \\
\>$n \times u$, $u \times n$ \>   multiplication of each item of a vector by a scalar \\
\>$\iota \; n$ \>\>  the vector of the first n integers starting from $0$ \\
%\footnote{$\iota \vec n$ is also defined and  produces an array of 
%cartesian coordinates~\cite{mul88}.}\\
\>$\pi \; v$ \> \>a scalar which is the product of the components of v \\
\>$k \;\take \;u$ \>\>	when $k \geq 0$ the vector of the first $k$ items of $u$) \\
\> \> \>and when $k<0$ the vector of the last $k$ items of $u$ \\
\>$k \;\drop \;u$ \>  \> when $k \geq 0$ the vector of $t_u - k$ last items of $u$) \\
\> \> \>and when $k<0$ the vector of the first $t_u - |k|$ items of $u$ \\
\end{tabbing}

\begin{definition}
Let A be an n-dimensional array and p a vector of integers.
If p is an index of A, 
\[             p \psi A = a[\gamma(s_A,p)], \]
   where 
\begin{eqnarray}
      \gamma(s_A,p) & = & x_{n-1}  \;\;\;\;\; \mbox{defined by 
the recurrence}    \nonumber \\ 
                 x_0 & = & p_0, \nonumber \\ 
              x_j & = & x_{j-1} * s_j + p_j, \; \; \; j=1,...,n-1. \nonumber
\end{eqnarray}

If $p$ is partial index of length $k < n,$
\[             p \psi A = B \]
where the shape of $B$ is
\[         s_B = k \drop s_A,    \] 
and for every index $q$ of $B$,
\[         q \psi B = (p \cat q) \psi A \]
\end{definition}
The definition uses the second form of specifying an array to define the result
of a partial index. For the index case, the function $\gamma(s,p)$ 
is used to convert an index $p$ to an
integer giving the location of the corresponding item 
in the row major order list of items of an array of shape $s$. The
recurrence computation for $\gamma$ is the one used in most compilers 
for converting an
index to a memory address~\cite{djr}.

\begin{corollary}
$<> \psi$ A = A.
\end{corollary}

The following theorem shows that a $\psi$ selection with a partial index can
be expressed as a composition of $\psi$ selections.

\begin{theorem}
Let A be an n-dimensional array and p a partial index so 
that $p = q \cat r.$ Then
\[                 p \psi A = r \psi (q \psi A) . \]

Proof: The proof is a consequence of the fact that for vectors u,v,w
\[           (u \cat v) \cat w = u \cat (v \cat w). \]
If we extend p to a full index by $p \cat p',$ then

\begin{eqnarray}
             p' \psi (p \psi A) & = & (p \cat p') \psi A \nonumber \\
                             & = & ((q \cat r) \cat p') \psi A \nonumber \\
                             &  =  & (q \cat (r \cat p')) \psi A \nonumber \\
                            &  =  & (r \cat p') \psi (q \psi A) \nonumber \\
                            &   = & p' \psi ( r \psi (q \psi A)) \nonumber \\
                          p \psi A & = & r \psi (q \psi A) \nonumber                   
\end{eqnarray}

\end{theorem}
{\em which completes the proof.}

We can now use {\em psi} to define other operations on arrays. 
For example, consider definitions of {\em take} and {\em drop} for multidimensional arrays.
\begin{definition}[take: $\take$]
Let A be an n-dimensional array, and k a non-negative integer
such that $0 \leq k < s_0$. Then
\[            k \take A = B \]
where
\[            s_B = <k> \cat (1 \drop s_A) \]
and for every index p of B,
\[            p \psi B = p \psi A . \]

\end{definition}
(In MoA $\take$ is also defined for negative integers and is generalized to any vector u with its
absolute value vector a partial index of A. The details are omitted here.)
\begin{definition}[reverse: $\Phi$]
 Let A be an n-dimensional array. Then
\[             s_{\Phi A} = s_A \]
and for every integer i, $0 \leq i < s_0, $
\[             <i> \psi \Phi A = <s_0 - i - 1> \psi A. \]
\end{definition}
This definition of $\Phi$ does a reversal of the 0th axis of A. 

Note
also that all operations are over the 0th axis. The operator $\Omega$~\cite{mul88} extends operations over all other dimensions.

\subsection*{Higher Order Operations}

Thus far operation on arrays, such as concatenation, rotation, etc., have been 
performed over their 0th dimensions.  We introduce the higher order binary 
operation $\Omega$, which is defined when its left argument is a unary or binary
operation and its right argument is a vector describing the dimension upon 
which operations are to be performed, or which sub-arrays are used in 
operations.  The dimension upon which operations are to be performed is often 
called the {\it axis} of operation.  The result of $\Omega$ is a unary or 
binary operation.

\subsection*{From Semantic to Operational Forms}

%Section with Psi  Correspondence Theorem and thinking in arrays *\

Consider the expression

\begin{eqnarray}
X & = & (2 \take ( \Phi A)) \times ( 1 \drop (\Phi A)) 
\end{eqnarray}
That is, take the first two planes after reversing A along the primary axis then multiply that array by the last two 
planes after reversing A. We want to concurrently perform both 5 by 4 multiplications. 
A is the array previously defined with shape $<3\;5\;4>$.
Suppose also that A is in shared memory and accessible to 2 processors.
First, we {\em Psi-Reduce} our expression to its Denotational Normal Form (DNF) or semantic 
normal form. This normal form can be used to prove the equivalence of  two array expressions.

Get shape: 
\begin{eqnarray}
\rho X & = & \rho (2 \take (\Phi A)) \nonumber \\
& = & 2 \cat ((1\drop (\rho ( \Phi A)))) \nonumber \\
&=& 2 \cat ((1 \drop (\rho  A))) \nonumber \\
&=& 2 \cat <\;5\;4> \nonumber \\
&=& <2\;4\;5>
\end{eqnarray}
Now that we have the {\em shape}, we can {Psi-Reduce}.
\[ \forall \;\;\; i,j,k \;\;\; \ni \;\;\; 0 \;\;\;\leq i,j,k < \;\;\;<2\;4\;5> \]
\[<i\;j\;k> \psi X \]
\begin{eqnarray}
& =&<i\;j\;k> \psi (2 \take ( \Phi A)) \times ( 1 \drop (\Phi A)) \nonumber \\
&=&<j\;k> \psi (<i> \psi (2 \take ( \Phi A)) \times ( 1 \drop (\Phi A)) )\nonumber \\
&= &<j\;k> \psi ((<i> \psi ( 2 \take (\Phi A))) \nonumber \\
&&\;\;\;\;\;\times ( <i> \psi ( 1 \drop (\Phi A)))) \nonumber \\
&= &<j\;k> \psi (( <i> \psi (\Phi A)) \times (<i+1> \psi (\Phi A))) \nonumber \\
&=&<j\;k> \psi ((<((\rho A)[0] - 1 - i)> \psi A ) \nonumber \\
&& \;\;\;\;\;\times  (<((\rho A)[0] - 1 -( i + 1))> \psi A ) )\nonumber \\
&=&<j\;k> \psi ((< 2-i> \psi A ) \times (<1-i>  \psi A) ) \nonumber \\
&=& (< (2-i)\; j\;k> \psi A )  \times (<(i-i) \; j\;k> \psi A ) 
 \end{eqnarray}
\normalsize
This is the DNF, or semantic normal form. To proceed further we must know more about the data, i.e. layout, and architecture. 
\subsection* {Applying the Psi Correspondence Theorem: PCT}
We now want to {\em transform} the DNF above to a {\em Generic} Operational Normal Form (ONF) or way 
to {\em build the code}.  Recall that $\gamma$ maps a {\em full} index , $\vec i$, of an arbitrary array $A$ to an 
offset from the start of A, $@A$ in memory,  denoted by $a[\gamma(\vec i;(\rho A))]$.
The PCT~\cite{muljen94},  generalizes the mapping function 
gamma, $\gamma$,   and turns {\em short} indices into start, stops, and strides.
Note that we still use {\em Psi-Reduction} but now just on vectors.  
We start by analyzing the shape of the result in conjunction with the indicies involved
in the computation and observe that the indices for $j$ and $k$ are contiguous allowing
us to only use the primary axis index, a short index, in the mapping.
Thus, let Y denote $(<2-i> \psi A ) \times ( <1-i> \psi A)$. Then the shape of Y, i.e.
\begin{eqnarray}
\rho Y= 1 \drop (\rho A) = 1 \drop <3\;5\;4> = <5\;4>
\end{eqnarray}
i.e. the bounds for $j$ and $k$  above.
Continuing with the PCT to create the {\em generic} ONF on p=2 processors, we use the index of the primary axis, i, $\ni\;\;0 \leq  i < p$, since $\rho X = <2\;4\;5>$.
Using $\gamma_{row}$ for row major ordering, Y is transformed into:
\small
\begin{eqnarray}
a[ (( \gamma(<2-i >; <1 \take (\rho A>))) \times \pi ( 1 \drop (\rho A))   )  + \iota \pi 1 \drop \rho A  ] \nonumber \\
\times a[ (( \gamma(<1-i >; <1 \take (\rho A>))) \times \pi ( 1 \drop (\rho A))   )  + \iota \pi 1 \drop \rho A  ] \nonumber 
\end{eqnarray}
Reducing, we get
\begin{eqnarray}
a[ (( \gamma(<2-i >; <3>))) \times \pi ( <5\;4>)  + \iota \pi <5\;4> ] \nonumber \\
\times a[ (( \gamma(<1-i >; <3>))) \times \pi ( <5\;4>)   )  + \iota \pi <5\;4>  ]\nonumber \\
=\;\;a[( (2-i) \times  20)  + \iota 20 ] \nonumber \\
\times a[( (i-1) \times 20 )  + \iota 20 ]
\end{eqnarray}
\normalsize
which transforms the indexing into start, stride, and stop, a universal hardware description and easiliy transformed
to the mnemonics used at any hardware level ~\cite{fftharry}. 

The  {\em iota} operation depicts  20 sequential 
accesses. If we wanted to map these operations to other than  scalar registers
we could conceptually add more dimensions to analyze prefetching, buffer sizes, vector registers, caches, etc.
Suppose in this example we have two vector registers to do the addition, both with length 4, we could
abstract this operation into a 5 by 4 where we can now analyze the cost of 5 vector register loads and
stores and determine if that is more cost effective than 20 scalar operations. Thus prior to code generation we
{\em could} analyze which menomnic to map to from the ONF. 
